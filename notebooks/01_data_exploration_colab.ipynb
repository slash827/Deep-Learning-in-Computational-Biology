{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25353c3a",
   "metadata": {},
   "source": [
    "# Phase 1: RNA-Protein Binding Prediction - Data Exploration (Colab Version)\n",
    "\n",
    "This Colab-ready notebook mirrors the local `01_data_exploration.ipynb` and adds setup for Google Colab (Drive mount, pip installs, and paths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad49ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Colab Setup: Install deps and (optionally) mount Google Drive\n",
    "import sys, os, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules or 'COLAB_GPU' in os.environ\n",
    "print('Running in Colab:', IN_COLAB)\n",
    "\n",
    "DRIVE_MOUNTED = False\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        DRIVE_MOUNTED = True\n",
    "        print('Drive mounted at /content/drive')\n",
    "    except Exception as e:\n",
    "        print('Drive mount skipped or failed:', e)\n",
    "        DRIVE_MOUNTED = False\n",
    "\n",
    "# Install minimal required packages (independent of any repo requirements.txt)\n",
    "if IN_COLAB:\n",
    "    pkgs = ['numpy', 'pandas', 'matplotlib', 'seaborn', 'tqdm', 'scipy', 'scikit-learn', 'torch']\n",
    "    print('Installing packages:', pkgs)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', *pkgs])\n",
    "\n",
    "# Base directories (project-independent)\n",
    "if IN_COLAB:\n",
    "    BASE_DIR = Path('/content/drive/MyDrive') if DRIVE_MOUNTED else Path('/content')\n",
    "else:\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "RUNS_DIR = BASE_DIR / 'dlcb_runs'  # where outputs will be saved\n",
    "DATA_DIR = BASE_DIR / 'data'       # where data files should exist or be uploaded to\n",
    "\n",
    "print('Base dir:', BASE_DIR)\n",
    "print('Runs dir:', RUNS_DIR)\n",
    "print('Data dir:', DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1 Local helper utilities (self-contained, no repo imports)\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def create_run_directory(base_dir, run_name):\n",
    "    p = Path(base_dir) / run_name\n",
    "    (p).mkdir(parents=True, exist_ok=True)\n",
    "    (p / 'models').mkdir(parents=True, exist_ok=True)\n",
    "    (p / 'plots').mkdir(parents=True, exist_ok=True)\n",
    "    return str(p)\n",
    "\n",
    "def save_training_config(cfg, run_dir):\n",
    "    with open(Path(run_dir) / 'config.json', 'w') as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "def save_training_summary(summary, run_dir):\n",
    "    with open(Path(run_dir) / 'summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "print('Local helpers ready (project-independent)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data paths and basic checks\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "print('Looking for data files in:', DATA_DIR)\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "required_files = ['training_seqs.txt', 'training_RBPs2.txt', 'training_data2.txt']\n",
    "for p in required_files:\n",
    "    full = Path(DATA_DIR)/p\n",
    "    print('-', p, 'exists:', full.exists())\n",
    "\n",
    "missing = [p for p in required_files if not (Path(DATA_DIR)/p).exists()]\n",
    "if missing:\n",
    "    print('\\nMissing files:', missing)\n",
    "    print('Options:')\n",
    "    print('  • Place them under', DATA_DIR)\n",
    "    print('  • Or run the previous cell to upload them directly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a171a8",
   "metadata": {},
   "source": [
    "### Optional: Upload data files directly (no Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you prefer to upload the 3 data files instead of using Drive\n",
    "try:\n",
    "    from google.colab import files  # type: ignore\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print('Please select files: training_seqs.txt, training_RBPs2.txt, training_data2.txt')\n",
    "    uploaded = files.upload()\n",
    "    for fname, content in uploaded.items():\n",
    "        out_path = DATA_DIR / fname\n",
    "        with open(out_path, 'wb') as f:\n",
    "            f.write(content)\n",
    "        print('Saved', out_path)\n",
    "except Exception as e:\n",
    "    print('Upload helper not available. If in Colab, ensure the environment is active. Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e39127",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries (Colab-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import signal\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For evaluation metrics\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# For progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Create run directory under RUNS_DIR\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f'notebook_phase1_{timestamp}'\n",
    "run_dir = create_run_directory(RUNS_DIR, run_name)\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "print(f'Run directory: {run_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1b1ac5",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9742b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory from earlier setup\n",
    "data_dir = str(DATA_DIR)\n",
    "\n",
    "# Load RNA sequences\n",
    "print('Loading RNA sequences...')\n",
    "rna_file = os.path.join(data_dir, 'training_seqs.txt')\n",
    "with open(rna_file, 'r') as f:\n",
    "    rna_sequences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f'Loaded {len(rna_sequences):,} RNA sequences')\n",
    "print('First 5 RNA sequences:')\n",
    "for i, seq in enumerate(rna_sequences[:5]):\n",
    "    print(f'  {i+1}: {seq}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "\n",
    "# Load protein sequences\n",
    "print('Loading protein sequences...')\n",
    "protein_file = os.path.join(data_dir, 'training_RBPs2.txt')\n",
    "with open(protein_file, 'r') as f:\n",
    "    protein_sequences = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f'Loaded {len(protein_sequences):,} protein sequences')\n",
    "print('First 3 protein sequences:')\n",
    "for i, seq in enumerate(protein_sequences[:3]):\n",
    "    print(f\"  {i+1}: {seq[:60]}{'...' if len(seq) > 60 else ''} (length: {len(seq)})\")\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "\n",
    "# Load a sample of binding scores to understand the format\n",
    "print('Examining binding scores format...')\n",
    "scores_file = os.path.join(data_dir, 'training_data2.txt')\n",
    "\n",
    "# Read first few lines to understand structure\n",
    "with open(scores_file, 'r') as f:\n",
    "    sample_lines = [f.readline().strip() for _ in range(5)]\n",
    "\n",
    "print('Sample binding score lines:')\n",
    "for i, line in enumerate(sample_lines):\n",
    "    scores = line.split()\n",
    "    print(f'  Line {i+1}: {len(scores)} scores, first 10: {scores[:10]}')\n",
    "\n",
    "print(f\"\\nData structure:\")\n",
    "print(f\"- Each line represents binding scores for one RNA sequence across all {len(protein_sequences)} proteins\")\n",
    "print(f\"- Total expected lines: {len(rna_sequences):,}\")\n",
    "print(f\"- Each line should have {len(protein_sequences)} scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sequence characteristics\n",
    "print('Analyzing sequence characteristics...')\n",
    "\n",
    "# RNA sequence analysis\n",
    "rna_lengths = [len(seq) for seq in rna_sequences]\n",
    "rna_nucleotide_counts = Counter(''.join(rna_sequences))\n",
    "\n",
    "# Protein sequence analysis\n",
    "protein_lengths = [len(seq) for seq in protein_sequences]\n",
    "protein_aa_counts = Counter(''.join(protein_sequences))\n",
    "\n",
    "# Display statistics\n",
    "print('\\nRNA Sequence Statistics:')\n",
    "print(f'  Count: {len(rna_sequences):,}')\n",
    "print(f'  Length - Min: {min(rna_lengths)}, Max: {max(rna_lengths)}, Mean: {np.mean(rna_lengths):.1f}')\n",
    "print(f'  Length - Median: {np.median(rna_lengths):.1f}, Std: {np.std(rna_lengths):.1f}')\n",
    "print(f'  Nucleotide composition: {dict(sorted(rna_nucleotide_counts.items()))}')\n",
    "\n",
    "print('\\nProtein Sequence Statistics:')\n",
    "print(f'  Count: {len(protein_sequences):,}')\n",
    "print(f'  Length - Min: {min(protein_lengths)}, Max: {max(protein_lengths)}, Mean: {np.mean(protein_lengths):.1f}')\n",
    "print(f'  Length - Median: {np.median(protein_lengths):.1f}, Std: {np.std(protein_lengths):.1f}')\n",
    "\n",
    "# Top 10 amino acids\n",
    "top_aa = sorted(protein_aa_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(f'  Top 10 amino acids: {top_aa}')\n",
    "\n",
    "# Visualize sequence length distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# RNA length distribution\n",
    "axes[0].hist(rna_lengths, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(np.mean(rna_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(rna_lengths):.1f}')\n",
    "axes[0].axvline(np.median(rna_lengths), color='orange', linestyle='--', label=f'Median: {np.median(rna_lengths):.1f}')\n",
    "axes[0].set_xlabel('RNA Sequence Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of RNA Sequence Lengths')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Protein length distribution\n",
    "axes[1].hist(protein_lengths, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[1].axvline(np.mean(protein_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(protein_lengths):.1f}')\n",
    "axes[1].axvline(np.median(protein_lengths), color='orange', linestyle='--', label=f'Median: {np.median(protein_lengths):.1f}')\n",
    "axes[1].set_xlabel('Protein Sequence Length')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Protein Sequence Lengths')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine optimal sequence lengths (95th percentile)\n",
    "rna_95th = np.percentile(rna_lengths, 95)\n",
    "protein_95th = np.percentile(protein_lengths, 95)\n",
    "\n",
    "print(f'\\nRecommended max lengths (95th percentile):')\n",
    "print(f'  RNA: {rna_95th:.0f}')\n",
    "print(f'  Protein: {protein_95th:.0f}')\n",
    "\n",
    "# Load and analyze binding scores (sample only due to large file size)\n",
    "print('Loading and analyzing binding scores...')\n",
    "\n",
    "# Load first 1000 lines of binding scores for analysis\n",
    "sample_scores = []\n",
    "with open(scores_file, 'r') as f:\n",
    "    for i in range(1000):  # Sample first 1000 RNA sequences\n",
    "        line = f.readline().strip()\n",
    "        if line:\n",
    "            scores = [float(x) for x in line.split()]\n",
    "            sample_scores.extend(scores)\n",
    "\n",
    "sample_scores = np.array(sample_scores)\n",
    "\n",
    "print(f'Binding Score Statistics (sample of {len(sample_scores):,} scores):')\n",
    "print(f'  Min: {sample_scores.min():.3f}')\n",
    "print(f'  Max: {sample_scores.max():.3f}')\n",
    "print(f'  Mean: {sample_scores.mean():.3f}')\n",
    "print(f'  Median: {np.median(sample_scores):.3f}')\n",
    "print(f'  Std: {sample_scores.std():.3f}')\n",
    "print(f'  25th percentile: {np.percentile(sample_scores, 25):.3f}')\n",
    "print(f'  75th percentile: {np.percentile(sample_scores, 75):.3f}')\n",
    "\n",
    "# Visualize binding score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(sample_scores, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0].axvline(sample_scores.mean(), color='red', linestyle='--', label=f'Mean: {sample_scores.mean():.3f}')\n",
    "axes[0].axvline(np.median(sample_scores), color='orange', linestyle='--', label=f'Median: {np.median(sample_scores):.3f}')\n",
    "axes[0].set_xlabel('Binding Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Binding Scores')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(sample_scores, patch_artist=True, \n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "axes[1].set_ylabel('Binding Score')\n",
    "axes[1].set_title('Box Plot of Binding Scores')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for data quality issues\n",
    "print(f'\\nData Quality Check:')\n",
    "print(f'  NaN values: {np.isnan(sample_scores).sum()}')\n",
    "print(f'  Infinite values: {np.isinf(sample_scores).sum()}')\n",
    "print(f'  Negative values: {(sample_scores < 0).sum()}')\n",
    "print(f'  Values > 10: {(sample_scores > 10).sum()}')\n",
    "\n",
    "# Normalize sample scores to see normalized distribution\n",
    "normalized_scores = (sample_scores - sample_scores.min()) / (sample_scores.max() - sample_scores.min())\n",
    "print(f'\\nNormalized scores (0-1 range):')\n",
    "print(f'  Min: {normalized_scores.min():.3f}, Max: {normalized_scores.max():.3f}')\n",
    "print(f'  Mean: {normalized_scores.mean():.3f}, Std: {normalized_scores.std():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80197621",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45990c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_rna_sequence(sequence, max_length=None):\n",
    "    \"\"\"One-hot encode RNA sequence.\"\"\"\n",
    "    nucleotide_mapping = {'A': 0, 'U': 1, 'G': 2, 'C': 3, 'T': 1, 'N': 4}\n",
    "    sequence_indices = []\n",
    "    for nucleotide in sequence.upper():\n",
    "        sequence_indices.append(nucleotide_mapping.get(nucleotide, 4))\n",
    "    if max_length:\n",
    "        if len(sequence_indices) > max_length:\n",
    "            sequence_indices = sequence_indices[:max_length]\n",
    "        else:\n",
    "            sequence_indices.extend([4] * (max_length - len(sequence_indices)))\n",
    "    num_nucleotides = 5\n",
    "    one_hot = np.zeros((len(sequence_indices), num_nucleotides))\n",
    "    for i, idx in enumerate(sequence_indices):\n",
    "        one_hot[i, idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def encode_protein_sequence(sequence, max_length=None):\n",
    "    \"\"\"One-hot encode protein sequence.\"\"\"\n",
    "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    aa_mapping = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "    aa_mapping['X'] = 20\n",
    "    sequence_indices = []\n",
    "    for aa in sequence.upper():\n",
    "        sequence_indices.append(aa_mapping.get(aa, 20))\n",
    "    if max_length:\n",
    "        if len(sequence_indices) > max_length:\n",
    "            sequence_indices = sequence_indices[:max_length]\n",
    "        else:\n",
    "            sequence_indices.extend([20] * (max_length - len(sequence_indices)))\n",
    "    num_amino_acids = 21\n",
    "    one_hot = np.zeros((len(sequence_indices), num_amino_acids))\n",
    "    for i, idx in enumerate(sequence_indices):\n",
    "        one_hot[i, idx] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Test encoding functions\n",
    "print('Testing encoding functions...')\n",
    "\n",
    "# Test RNA encoding\n",
    "test_rna = rna_sequences[0]\n",
    "encoded_rna = encode_rna_sequence(test_rna, max_length=50)\n",
    "print(f'Original RNA: {test_rna}')\n",
    "print(f'Encoded shape: {encoded_rna.shape}')\n",
    "print('First 5 positions (one-hot):')\n",
    "for i in range(5):\n",
    "    print(f\"  Position {i}: {encoded_rna[i]} -> {test_rna[i] if i < len(test_rna) else 'PAD'}\")\n",
    "\n",
    "print('\\n' + '-'*50)\n",
    "\n",
    "# Test protein encoding\n",
    "test_protein = protein_sequences[0]\n",
    "encoded_protein = encode_protein_sequence(test_protein, max_length=100)\n",
    "print(f\"Original protein: {test_protein[:50]}...\")\n",
    "print(f'Encoded shape: {encoded_protein.shape}')\n",
    "print('First 5 positions (one-hot):')\n",
    "for i in range(5):\n",
    "    print(f\"  Position {i}: {encoded_protein[i]} -> {test_protein[i] if i < len(test_protein) else 'PAD'}\")\n",
    "\n",
    "# Visualize encoding example\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "axes[0].imshow(encoded_rna[:20].T, cmap='Blues', aspect='auto')\n",
    "axes[0].set_xlabel('Sequence Position')\n",
    "axes[0].set_ylabel('Nucleotide (A,U,G,C,N)')\n",
    "axes[0].set_title('RNA One-Hot Encoding (First 20 positions)')\n",
    "axes[0].set_yticks(range(5))\n",
    "axes[0].set_yticklabels(['A', 'U', 'G', 'C', 'N'])\n",
    "\n",
    "axes[1].imshow(encoded_protein[:20].T, cmap='Reds', aspect='auto')\n",
    "axes[1].set_xlabel('Sequence Position')\n",
    "axes[1].set_ylabel('Amino Acid Index')\n",
    "axes[1].set_title('Protein One-Hot Encoding (First 20 positions)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09071575",
   "metadata": {},
   "source": [
    "## 4. Dataset and DataLoader Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ecdf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNAProteinDataset(Dataset):\n",
    "    def __init__(self, rna_sequences, protein_sequences, binding_scores,\n",
    "                 rna_max_length=50, protein_max_length=500, normalize_scores=True):\n",
    "        self.rna_sequences = rna_sequences\n",
    "        self.protein_sequences = protein_sequences\n",
    "        self.binding_scores = binding_scores.copy()\n",
    "        self.rna_max_length = rna_max_length\n",
    "        self.protein_max_length = protein_max_length\n",
    "        if normalize_scores and len(self.binding_scores) > 0:\n",
    "            min_score = self.binding_scores.min()\n",
    "            max_score = self.binding_scores.max()\n",
    "            if max_score > min_score:\n",
    "                self.binding_scores = (self.binding_scores - min_score) / (max_score - min_score)\n",
    "        print('Pre-encoding sequences...')\n",
    "        self.rna_encoded = []\n",
    "        self.protein_encoded = []\n",
    "        for i, (rna_seq, protein_seq) in enumerate(zip(rna_sequences, protein_sequences)):\n",
    "            if i % 1000 == 0:\n",
    "                print(f'Encoded {i}/{len(rna_sequences)} sequences')\n",
    "            self.rna_encoded.append(encode_rna_sequence(rna_seq, self.rna_max_length))\n",
    "            self.protein_encoded.append(encode_protein_sequence(protein_seq, self.protein_max_length))\n",
    "        print(f'Encoding complete. Dataset size: {len(self.rna_encoded)}')\n",
    "    def __len__(self):\n",
    "        return len(self.rna_sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        rna_tensor = torch.FloatTensor(self.rna_encoded[idx])\n",
    "        protein_tensor = torch.FloatTensor(self.protein_encoded[idx])\n",
    "        score_tensor = torch.FloatTensor([self.binding_scores[idx]])\n",
    "        return {'rna': rna_tensor, 'protein': protein_tensor, 'score': score_tensor}\n",
    "\n",
    "\n",
    "def load_subset_data(data_dir, subset_size=200):\n",
    "    print(f'Loading subset of {subset_size} RNA sequences...')\n",
    "    rna_subset = rna_sequences[:subset_size]\n",
    "    binding_scores = []\n",
    "    with open(os.path.join(data_dir, 'training_data2.txt'), 'r') as f:\n",
    "        for i in range(subset_size):\n",
    "            line = f.readline().strip()\n",
    "            if line:\n",
    "                scores = [float(x) for x in line.split()]\n",
    "                binding_scores.extend(scores)\n",
    "    binding_scores = np.array(binding_scores)\n",
    "    rna_protein_pairs = []\n",
    "    protein_rna_pairs = []\n",
    "    final_scores = []\n",
    "    scores_per_rna = len(protein_sequences)\n",
    "    for rna_idx, rna_seq in enumerate(rna_subset):\n",
    "        start_idx = rna_idx * scores_per_rna\n",
    "        end_idx = start_idx + scores_per_rna\n",
    "        if end_idx <= len(binding_scores):\n",
    "            rna_scores = binding_scores[start_idx:end_idx]\n",
    "            for protein_idx, protein_seq in enumerate(protein_sequences):\n",
    "                rna_protein_pairs.append(rna_seq)\n",
    "                protein_rna_pairs.append(protein_seq)\n",
    "                final_scores.append(rna_scores[protein_idx])\n",
    "    print(f'Created {len(rna_protein_pairs)} RNA-protein pairs')\n",
    "    return rna_protein_pairs, protein_rna_pairs, np.array(final_scores)\n",
    "\n",
    "subset_rna, subset_protein, subset_scores = load_subset_data(data_dir, subset_size=200)\n",
    "\n",
    "print('Subset data summary:')\n",
    "print(f'  RNA-protein pairs: {len(subset_rna)}')\n",
    "print(f'  Score range: {subset_scores.min():.3f} - {subset_scores.max():.3f}')\n",
    "print(f'  Score mean: {subset_scores.mean():.3f}')\n",
    "\n",
    "RNA_MAX_LENGTH = 50\n",
    "PROTEIN_MAX_LENGTH = 500\n",
    "\n",
    "dataset = RNAProteinDataset(\n",
    "    subset_rna, subset_protein, subset_scores,\n",
    "    rna_max_length=RNA_MAX_LENGTH,\n",
    "    protein_max_length=PROTEIN_MAX_LENGTH\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('Dataset split:')\n",
    "print(f'  Training: {len(train_dataset)} samples')\n",
    "print(f'  Validation: {len(val_dataset)} samples')\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'Data loaders created with batch size: {batch_size}')\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "print('Sample batch shapes:')\n",
    "print(f\"  RNA: {sample_batch['rna'].shape}\")\n",
    "print(f\"  Protein: {sample_batch['protein'].shape}\")\n",
    "print(f\"  Scores: {sample_batch['score'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35c60f",
   "metadata": {},
   "source": [
    "## 5. Basic Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c523fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, rna_input_size=5, protein_input_size=21,\n",
    "                 rna_hidden_size=128, protein_hidden_size=128,\n",
    "                 num_layers=2, dropout=0.2, fusion_hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.rna_hidden_size = rna_hidden_size\n",
    "        self.protein_hidden_size = protein_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rna_lstm = nn.LSTM(\n",
    "            input_size=rna_input_size,\n",
    "            hidden_size=rna_hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.protein_lstm = nn.LSTM(\n",
    "            input_size=protein_input_size,\n",
    "            hidden_size=protein_hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        fusion_input_size = rna_hidden_size * 2 + protein_hidden_size * 2\n",
    "        self.fusion_layers = nn.Sequential(\n",
    "            nn.Linear(fusion_input_size, fusion_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fusion_hidden_size, fusion_hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fusion_hidden_size // 2, 1)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "                if 'bias_ih' in name:\n",
    "                    n = param.size(0)\n",
    "                    param.data[n//4:n//2].fill_(1)\n",
    "    def forward(self, rna, protein):\n",
    "        batch_size = rna.size(0)\n",
    "        rna_output, (rna_hidden, _) = self.rna_lstm(rna)\n",
    "        rna_hidden = rna_hidden.view(self.num_layers, 2, batch_size, self.rna_hidden_size)\n",
    "        rna_representation = torch.cat([rna_hidden[-1, 0], rna_hidden[-1, 1]], dim=1)\n",
    "        protein_output, (protein_hidden, _) = self.protein_lstm(protein)\n",
    "        protein_hidden = protein_hidden.view(self.num_layers, 2, batch_size, self.protein_hidden_size)\n",
    "        protein_representation = torch.cat([protein_hidden[-1, 0], protein_hidden[-1, 1]], dim=1)\n",
    "        rna_representation = self.dropout(rna_representation)\n",
    "        protein_representation = self.dropout(protein_representation)\n",
    "        combined = torch.cat([rna_representation, protein_representation], dim=1)\n",
    "        binding_score = self.fusion_layers(combined)\n",
    "        return binding_score\n",
    "\n",
    "# Create model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model = BasicLSTM().to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Model created:')\n",
    "print(f'  Total parameters: {total_params:,}')\n",
    "print(f'  Trainable parameters: {trainable_params:,}')\n",
    "print(f'  Model size: ~{total_params * 4 / 1024**2:.1f} MB (FP32)')\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    rna_test = sample_batch['rna'].to(device)\n",
    "    protein_test = sample_batch['protein'].to(device)\n",
    "    output = model(rna_test, protein_test)\n",
    "    print('Forward pass test:')\n",
    "    print('  Input RNA shape:', rna_test.shape)\n",
    "    print('  Input protein shape:', protein_test.shape)\n",
    "    print('  Output shape:', output.shape)\n",
    "    print('  Sample predictions:', output[:5].flatten().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab093d3f",
   "metadata": {},
   "source": [
    "## 6. Training Loop Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708feead",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "training_config = {\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 5,\n",
    "    'min_delta': 1e-4,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'lr_scheduler_patience': 3,\n",
    "    'lr_scheduler_factor': 0.5,\n",
    "    'num_epochs': 5,  # Keep small for Colab demo; increase as needed\n",
    "    'device': str(device),\n",
    "    'run_name': run_name,\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "save_training_config(training_config, run_dir)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=training_config['learning_rate'], \n",
    "                      weight_decay=training_config['weight_decay'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=training_config['lr_scheduler_factor'], \n",
    "    patience=training_config['lr_scheduler_patience'], verbose=True\n",
    ")\n",
    "\n",
    "def calculate_pearson_correlation(y_true, y_pred):\n",
    "    correlation, _ = pearsonr(y_true.flatten(), y_pred.flatten())\n",
    "    return correlation if not np.isnan(correlation) else 0.0\n",
    "\n",
    "class TrainingInterruptHandler:\n",
    "    def __init__(self):\n",
    "        self.interrupted = False\n",
    "    def interrupt(self):\n",
    "        self.interrupted = True\n",
    "        print('\\nTraining interrupted! Finishing current epoch...')\n",
    "\n",
    "interrupt_handler = TrainingInterruptHandler()\n",
    "\n",
    "def train_epoch_enhanced(model, train_loader, criterion, optimizer, device, interrupt_handler, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    progress_bar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        if interrupt_handler.interrupted:\n",
    "            print(f'Training interrupted at batch {batch_idx}')\n",
    "            break\n",
    "        rna = batch['rna'].to(device)\n",
    "        protein = batch['protein'].to(device)\n",
    "        targets = batch['score'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(rna, protein)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        all_predictions.append(predictions.detach().cpu().numpy())\n",
    "        all_targets.append(targets.detach().cpu().numpy())\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float('inf'))\n",
    "        progress_bar.set_postfix({'Loss': f'{loss.item():.4f}', 'Grad Norm': f'{grad_norm:.3f}'})\n",
    "    if all_predictions:\n",
    "        predictions = np.concatenate(all_predictions, axis=0)\n",
    "        targets = np.concatenate(all_targets, axis=0)\n",
    "        correlation = calculate_pearson_correlation(targets, predictions)\n",
    "    else:\n",
    "        correlation = 0.0\n",
    "    return total_loss / len(train_loader) if len(train_loader) > 0 else float('inf'), correlation\n",
    "\n",
    "\n",
    "def validate_epoch_enhanced(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validating'):\n",
    "            rna = batch['rna'].to(device)\n",
    "            protein = batch['protein'].to(device)\n",
    "            targets = batch['score'].to(device)\n",
    "            predictions = model(rna, protein)\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    predictions = np.concatenate(all_predictions, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    correlation = calculate_pearson_correlation(targets, predictions)\n",
    "    return total_loss / len(val_loader), correlation, predictions, targets\n",
    "\n",
    "print('Training setup complete')\n",
    "\n",
    "num_epochs = training_config['num_epochs']\n",
    "train_losses, val_losses = [], []\n",
    "train_correlations, val_correlations = [], []\n",
    "best_val_correlation = -np.inf\n",
    "best_model_state = None\n",
    "epochs_without_improvement = 0\n",
    "patience = training_config['patience']\n",
    "min_delta = training_config['min_delta']\n",
    "\n",
    "print(f'Starting training for {num_epochs} epochs on {device}...')\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        if interrupt_handler.interrupted:\n",
    "            print(f'Training stopped at epoch {epoch+1} due to interruption')\n",
    "            break\n",
    "        train_loss, train_corr = train_epoch_enhanced(\n",
    "            model, train_loader, criterion, optimizer, device, \n",
    "            interrupt_handler, training_config['max_grad_norm']\n",
    "        )\n",
    "        if interrupt_handler.interrupted:\n",
    "            print(f'Epoch {epoch+1} interrupted during training')\n",
    "            break\n",
    "        val_loss, val_corr, val_predictions, val_targets = validate_epoch_enhanced(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        scheduler.step(val_corr)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_correlations.append(train_corr)\n",
    "        val_correlations.append(val_corr)\n",
    "        improvement = val_corr - best_val_correlation\n",
    "        if improvement > min_delta:\n",
    "            epochs_without_improvement = 0\n",
    "            best_val_correlation = val_corr\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f'New best model! Correlation improved by {improvement:.4f}')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - LR: {lr:.2e}')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        print(f'  Train Corr: {train_corr:.4f}, Val Corr: {val_corr:.4f}')\n",
    "        print(f'  Best Val Corr: {best_val_correlation:.4f}')\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "        print()\n",
    "except KeyboardInterrupt:\n",
    "    print(f'Interrupted at epoch {len(train_losses)+1}')\n",
    "    interrupt_handler.interrupt()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f'Loaded best model (correlation: {best_val_correlation:.4f})')\n",
    "\n",
    "training_summary = {\n",
    "    'best_val_correlation': best_val_correlation,\n",
    "    'best_epoch': int(np.argmax(val_correlations)) if val_correlations else 0,\n",
    "    'final_train_correlation': train_correlations[-1] if train_correlations else 0,\n",
    "    'final_val_correlation': val_correlations[-1] if val_correlations else 0,\n",
    "    'total_epochs': len(train_losses),\n",
    "    'total_training_time': total_time,\n",
    "    'interrupted': interrupt_handler.interrupted,\n",
    "    'early_stopped': epochs_without_improvement >= patience,\n",
    "    'training_config': training_config,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_correlations': train_correlations,\n",
    "    'val_correlations': val_correlations\n",
    "}\n",
    "\n",
    "save_training_summary(training_summary, run_dir)\n",
    "print('Training completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087345b8",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353378ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set with best model\n",
    "model.eval()\n",
    "val_loss, val_corr, final_predictions, final_targets = validate_epoch_enhanced(model, val_loader, criterion, device)\n",
    "\n",
    "# Calculate additional metrics\n",
    "mse = mean_squared_error(final_targets.flatten(), final_predictions.flatten())\n",
    "mae = mean_absolute_error(final_targets.flatten(), final_predictions.flatten())\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('Final Model Performance:')\n",
    "print(f'  Validation Loss (MSE): {val_loss:.4f}')\n",
    "print(f'  Pearson Correlation: {val_corr:.4f}')\n",
    "print(f'  Mean Absolute Error: {mae:.4f}')\n",
    "print(f'  Root Mean Square Error: {rmse:.4f}')\n",
    "\n",
    "# Model inference speed test\n",
    "print('\\nInference Speed Test:')\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(val_loader))\n",
    "    rna_test = test_batch['rna'].to(device)\n",
    "    protein_test = test_batch['protein'].to(device)\n",
    "    for _ in range(5):\n",
    "        _ = model(rna_test, protein_test)\n",
    "    start_time = time.time()\n",
    "    num_runs = 20\n",
    "    for _ in range(num_runs):\n",
    "        _ = model(rna_test, protein_test)\n",
    "    end_time = time.time()\n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    samples_per_second = rna_test.size(0) / avg_time\n",
    "\n",
    "print(f'  Average inference time: {avg_time*1000:.2f}ms per batch')\n",
    "print(f'  Throughput: {samples_per_second:.1f} samples/second')\n",
    "\n",
    "print('\\nPhase 1 Summary:')\n",
    "print('  Model: Basic Bidirectional LSTM')\n",
    "print(f'  Dataset: {len(subset_rna)} RNA-protein pairs')\n",
    "print(f'  Training time: {total_time:.1f}s')\n",
    "print(f'  Best validation correlation: {best_val_correlation:.4f}')\n",
    "print(f'  Final validation correlation: {val_corr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4086f8",
   "metadata": {},
   "source": [
    "## 8. Results Visualization and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_dir = os.path.join(run_dir, 'plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Training/Validation Loss\n",
    "axes[0, 0].plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
    "axes[0, 0].plot(val_losses, label='Validation Loss', color='orange', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss (MSE)')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training/Validation Correlation\n",
    "axes[0, 1].plot(train_correlations, label='Training Correlation', color='blue', linewidth=2)\n",
    "axes[0, 1].plot(val_correlations, label='Validation Correlation', color='orange', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Pearson Correlation')\n",
    "axes[0, 1].set_title('Training and Validation Correlation')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Predictions vs Targets Scatter Plot\n",
    "axes[1, 0].scatter(final_targets.flatten(), final_predictions.flatten(), alpha=0.6, s=20)\n",
    "min_val = min(final_targets.min(), final_predictions.min())\n",
    "max_val = max(final_targets.max(), final_predictions.max())\n",
    "axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "correlation = calculate_pearson_correlation(final_targets, final_predictions)\n",
    "axes[1, 0].set_xlabel('True Binding Scores')\n",
    "axes[1, 0].set_ylabel('Predicted Binding Scores')\n",
    "axes[1, 0].set_title(f'Predictions vs Targets\\nPearson Correlation: {correlation:.4f}')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals Plot\n",
    "residuals = final_targets.flatten() - final_predictions.flatten()\n",
    "axes[1, 1].scatter(final_predictions.flatten(), residuals, alpha=0.6, s=20)\n",
    "axes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Predicted Binding Scores')\n",
    "axes[1, 1].set_ylabel('Residuals (True - Predicted)')\n",
    "axes[1, 1].set_title(f'Residuals Plot\\nMean: {residuals.mean():.4f}, Std: {residuals.std():.4f}')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Phase 1 Training Results - {run_name}', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "main_plot_path = os.path.join(plots_dir, 'training_results.png')\n",
    "plt.savefig(main_plot_path, dpi=300, bbox_inches='tight')\n",
    "print('Main results plot saved to:', main_plot_path)\n",
    "plt.show()\n",
    "\n",
    "# Save model for future use\n",
    "model_save_path = os.path.join(run_dir, 'models', 'phase1_basic_lstm_model.pth')\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'rna_input_size': 5,\n",
    "        'protein_input_size': 21,\n",
    "        'rna_hidden_size': 128,\n",
    "        'protein_hidden_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.2,\n",
    "        'fusion_hidden_size': 256\n",
    "    },\n",
    "    'training_history': training_summary,\n",
    "    'sequence_lengths': {\n",
    "        'rna_max_length': RNA_MAX_LENGTH,\n",
    "        'protein_max_length': PROTEIN_MAX_LENGTH\n",
    "    },\n",
    "    'final_performance': {\n",
    "        'correlation': float(correlation),\n",
    "        'mse': float(mean_squared_error(final_targets.flatten(), final_predictions.flatten())),\n",
    "        'mae': float(mean_absolute_error(final_targets.flatten(), final_predictions.flatten()))\n",
    "    }\n",
    "}, model_save_path)\n",
    "\n",
    "print('Model saved to:', model_save_path)\n",
    "print('All outputs saved to:', run_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
